# api/main.py
import os
os.environ["JAVA_HOME"] = "/opt/homebrew/Cellar/openjdk@17/17.0.15/libexec/openjdk.jdk/Contents/Home"  # e.g. /usr/lib/jvm/java-11-openjdk-amd64 or $HOME/jdk/openjdk-11
os.environ["PATH"] = f"{os.environ['JAVA_HOME']}/bin:" + os.environ["PATH"]
os.environ["SPARK_VERSION"] = "3.5"

from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pyspark.sql import DataFrame
from pyspark.sql import SparkSession

from pyspark.sql import SparkSession, functions as F
from pydeequ.suggestions import ConstraintSuggestionRunner, DEFAULT
from pydeequ.checks import Check, CheckLevel
from pydeequ.verification import VerificationSuite, VerificationResult

app = FastAPI(title="Deequ Suggester API")
from starlette.middleware.base import BaseHTTPMiddleware

# ===== LLM client (OpenAI-compatible) =====
import os, re
from typing import Tuple

LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://localhost:8000/v1")   # e.g., vLLM/TGI/OpenAI router
LLM_MODEL    = os.getenv("LLM_MODEL", "meta-llama/Llama-3.3-70B-Instruct")
LLM_API_KEY  = os.getenv("LLM_API_KEY", "sk-set-me")

try:
    from openai import OpenAI  # pip install openai>=1.0
    _llm = OpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)
except Exception:
    _llm = None

async def log_requests(request, call_next):
    print(f"ğŸ”µ {request.method} {request.url}")
    response = await call_next(request)
    print(f"ğŸŸ¢ {response.status_code}")
    return response

app.add_middleware(BaseHTTPMiddleware, dispatch=log_requests)

# CORSï¼ˆå¼€å‘é˜¶æ®µå…ˆæ”¾å¼€ï¼›ä¸Šçº¿åæ”¹æˆä½ çš„å‰ç«¯åŸŸåï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== Lazy Spark =====
_SPARK = None
def spark():
    global _SPARK
    if _SPARK is None:
        _SPARK = (
            SparkSession.builder
            .appName("PyDeequAPI")
            .config("spark.jars.packages", "com.amazon.deequ:deequ:2.0.9-spark-3.5")
            .getOrCreate()
        )
    return _SPARK

# ===== è¯·æ±‚/å“åº”æ¨¡å‹ï¼ˆç²¾ç®€ç‰ˆï¼‰=====
from pydantic import BaseModel

class SuggestRequest(BaseModel):
    path: str                          # åªè¦è·¯å¾„ï¼ˆæŒ‰ Parquet è¯»å–ï¼‰
    key_cols: Optional[List[str]] = [] # å¯é€‰ï¼šæ‹¼ä¸ªå”¯ä¸€é”®åˆ—

class CheckRow(BaseModel):
    id: str
    column: str
    description: Optional[str] = ""
    rule: Optional[str] = ""
    code: str
    include: bool = True
    current_value: Optional[str] = None

class TranspileRequest(BaseModel):
    rows: List[CheckRow]
    force_all: bool = False
    model: Optional[str] = None  # optional per-request model override

class TranspileResponse(BaseModel):
    rows: List[CheckRow]
    errors: List[Dict[str, Any]] = []

class GenerateRequest(BaseModel):
    rows: List[CheckRow]
    check_name: str = "AutoGenerated"
    level: str = "Error"  # or "Warning"

class VerifyCodeRequest(BaseModel):
    path: str          # è¦éªŒè¯çš„æ•°æ® Parquet è·¯å¾„
    code: str          # /generate è¿”å›çš„ final code å­—ç¬¦ä¸²

class VerifyCodeResponse(BaseModel):
    total: int
    success: int
    failure: int
    per_constraint: List[Dict[str, Any]]  # æŒ‰é¡ºåºåˆ—å‡ºæ¯æ¡çº¦æŸçš„çŠ¶æ€
    failures: List[Dict[str, Any]]

class GenerateAndVerifyOnceRequest(BaseModel):
    path: str
    key_cols: Optional[List[str]] = []
    rows: List[CheckRow]
    check_name: str = "AutoGenerated"
    level: str = "Error"

# ===== helpers =====
    
# ===== Simple DF cache (optional, but speeds things up) =====
DF_CACHE: Dict[str, DataFrame] = {}

def _load_df(req: SuggestRequest):
    sp = spark()
    df = sp.read.parquet(req.path)
    if req.key_cols:
        cols = [c for c in req.key_cols if c in df.columns]
        if cols:
            df = df.withColumn("Key_col", F.concat_ws("_", *[F.col(c) for c in cols]))
    # cache by path
    DF_CACHE[req.path] = df
    return df

def _group_suggestions(
    df,                      # pyspark.sql.DataFrame
    suggestionResult: Dict[str, Any],
    key_cols: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    
    rows: List[Dict[str, Any]] = []

    # Existing Deequ suggestions
    for s in suggestionResult.get("constraint_suggestions", []):
        col = s.get("column_name") or "<DATASET>"
        rows.append({
            "id": f"{col}:{s.get('constraint_name')}",
            "column": col,
            "description": s.get("description"),
            "rule": s.get("rule_description") or s.get("suggesting_rule"),
            "code": s.get("code_for_constraint"),
            "current_value": s.get("current_value"),
            "include": True,
        })

    # Extra checks for key columns
    if key_cols:
        # Uniqueness: count of distinct non-null / total count
        uniqueness = (
            df.agg((F.countDistinct(F.col("Key_col")) / F.count(F.lit(1))).alias("uniqueness"))
            .collect()[0]["uniqueness"]
        )
        rows.append({
            'id': 'key_cols:UniquenessConstraint(Uniqueness(List(key_cols),None,None))',
            'column': 'key_cols',
            'description': "'key_cols' is unique",
            'rule': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint',
            'code': '.isUnique("key_cols")',
            'current_value': f'ApproxDistinctness: {uniqueness}',
            'include': True
        })

        # Completeness: count of non-null / total count
        completeness = (
            df.filter(F.col(col).isNotNull())
            .count() / df.count()
        )
        rows.append({
            'id': 'key_cols:CompletenessConstraint(Completeness(key_cols,None,None))',
            'column': 'key_cols',
            'description': "'key_cols' is not null",
            'rule': 'If a column is complete in the sample, we suggest a NOT NULL constraint',
            'code': '.isComplete("key_cols")',
            'current_value': f'Completeness: {completeness}',
            'include': True
        })
    
    # Add row count (size) check
    rows.append({
        'id': '<DATASET>:SizeConstraint(Size(None,None))',
        'column': 'DATASET',
        'description': 'Dataset should contain at least one row',
        'rule': 'we suggest adding a Size > 0 constraint',
        'code': '.hasSize(lambda x: x > 0)',
        'current_value': f'Size: {df.count()}',
        'include': True
    })

    # Schema
    schema_list = df.dtypes
    all_cols = [c for c, _ in schema_list]
    dtype_map = dict(schema_list)
    suggested_cols = list(dict.fromkeys([r.get("column") for r in rows if r.get("column")]))

    # For columns not included in Deequ auto suggestions at all, add a placeholder rule
    for col in all_cols:
        if col not in suggested_cols:
            rows.append({
                'id': f'{col}',
                'column': f'{col}',
                'description': f"'{col}' no defined rules",
                'rule': ' ',
                'code': ' ',
                'current_value': ' ',
                'include': True
            })

    return rows

def _run_check_code(df, final_code: Any):
    """
    Execute Deequ checks against df.

    final_code can be:
      - a string like:
          "Check(spark, CheckLevel.Error, 'Name')\n    .isComplete('A')\n    .isNonNegative('AMOUNT')"
      - OR a pydeequ.checks.Check object (already constructed)
    Returns: total, success, failure, flat_results, failures_only
    """
    # If it's a string, eval it into a Check object (safe, restricted context)
    if isinstance(final_code, str):
        ctx = {
            "Check": Check,
            "CheckLevel": CheckLevel,
            "spark": spark,   # NOTE: in your generated code you call: Check(spark, ...)
        }
        try:
            check = eval(final_code, {}, ctx)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Invalid generated code: {e}")
    elif isinstance(final_code, Check):
        check = final_code
    else:
        raise HTTPException(status_code=400, detail="final_code must be a string or a pydeequ Check")

    # Run verification
    result = (
        VerificationSuite(spark())
        .onData(df)
        .addCheck(check)
        .run()
    )
    res_json = VerificationResult.checkResultsAsJson(spark(), result)

    flat: List[Dict[str, Any]] = []
    for chk in res_json.get("checkResults", []):
        flat.extend(chk.get("constraintResults", []))

    total = len(flat)
    success = sum(1 for c in flat if c.get("status") == "Success")
    failure = total - success

    failures: List[Dict[str, Any]] = []
    for c in flat:
        if c.get("status") == "Failure":
            failures.append({
                "constraint": c.get("constraint", ""),
                "column": c.get("column", ""),
                "message": c.get("message", ""),
                "metric": c.get("metric", ""),
                "actualValue": c.get("actualValue", ""),
            })

    return total, success, failure, flat, failures

def _build_prompt(desc: str) -> str:
    return f"""
You are an expert PyDeequ developer.
Your task: convert a natural-language *description* of a data quality check into the corresponding **PyDeequ Check API code**.

Strict rules:
1. Always use valid PyDeequ syntax.
2. Always return only the code (starting with a dot, no explanation).
3. Always choose the correct function name:
   - use `.isContainedIn(col, [values])` for value range checks.
   - use `.isComplete(col)` for not-null checks.
   - use `.hasPattern(col, "regex")` for pattern checks.
   - use `.hasMin`, `.hasMax`, `.hasMean`, etc. for numeric stats.
4. If percentage or threshold is mentioned, use a lambda condition and descriptive message.
5. Follow this format exactly:

Examples:
'FILE_AIRBAG_CODE' has value range 'N', 'Y', 'U' -> .isContainedIn("FILE_AIRBAG_CODE", ["N","Y","U"])
'FILE_AIRBAG_CODE' is not null -> .isComplete("FILE_AIRBAG_CODE")
'FILE_AIRBAG_CODE' has value range 'N' for at least 94.0% of values -> .isContainedIn("FILE_AIRBAG_CODE", ["N"], lambda x: x >= 0.94, "It should be above 0.94!")

Do NOT invent new functions. Only use existing PyDeequ functions.

DEEQU DESCRIPTION:
{desc}

Return only the PyDeequ check code:
""".strip()

def _sanitize_llm_code(text: str) -> str:
    """
    Keep only the first line that starts with a dot (e.g., `.isContainedIn(...)`).
    Strip code fences or extra text if any model adds them.
    """
    t = text.strip()
    # drop code fences if present
    if t.startswith("```"):
        t = t.strip("`").strip()
    # take first line starting with '.'
    for line in t.splitlines():
        s = line.strip()
        if s.startswith("."):
            return s
    # fallback: return entire trimmed string (last resort)
    return t

def _llm_transpile(desc: str, model_override: str | None = None) -> str:
    if not _llm:
        raise RuntimeError("LLM client not initialized. Set LLM_BASE_URL/LLM_API_KEY or install 'openai'.")
    prompt = _build_prompt(desc)
    resp = _llm.chat.completions.create(
        model=model_override or LLM_MODEL,
        temperature=0.0,
        max_tokens=128,
        messages=[{"role": "user", "content": prompt}],
    )
    raw = resp.choices[0].message.content or ""
    return _sanitize_llm_code(raw)

# ===== API è·¯ç”± =====
@app.post("/suggest")
def suggest(req: SuggestRequest):
    try:
        df = _load_df(req)
        suggestionResult = (
            ConstraintSuggestionRunner(spark())
            .onData(df)
            .addConstraintRule(DEFAULT())
            .run()
        )
        rows = _group_suggestions(df, suggestionResult, key_cols = req.key_cols)
        schema = [{"name": f.name, "type": f.dataType.simpleString()} for f in df.schema.fields]
        return {"rows": rows, "row_count": len(rows), "schema": schema}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/transpile", response_model=TranspileResponse)
def transpile(req: TranspileRequest):
    updated: List[CheckRow] = []
    errors: List[Dict[str, Any]] = []

    for r in req.rows:
        # Only fill if force_all or code is empty/blank
        needs_llm = req.force_all or (not r.code or r.code.strip() == "" or r.code.strip() == "-")
        if not needs_llm:
            updated.append(r)
            continue

        desc = r.description or ""
        if not desc.strip():
            # Nothing to convert
            updated.append(r)
            continue

        try:
            code = _llm_transpile(desc, model_override=req.model)
            # minimal guard: ensure it starts with '.'
            if not code.strip().startswith("."):
                raise ValueError(f"LLM returned non-dot-leading code: {code!r}")
            updated.append(CheckRow(**{**r.dict(), "code": code}))
        except Exception as e:
            errors.append({"id": r.id, "error": str(e)})
            updated.append(r)  # keep original row

    return TranspileResponse(rows=updated, errors=errors)

@app.post("/generate")
def generate(req: GenerateRequest):
    selected = [r for r in req.rows if r.include and r.code]
    if not selected:
        return {"code": f"Check(spark, CheckLevel.{req.level}, {req.check_name!r})"}
    joined = "\n    ".join(r.code.strip() for r in selected)
    final_code = f"Check(spark, CheckLevel.{req.level}, {req.check_name!r})\n    {joined}"
    return {"code": final_code}

@app.post("/verify_code", response_model=VerifyCodeResponse)
def verify_code(req: VerifyCodeRequest):
    try:
        df = DF_CACHE.get(req.path)
        total, success, failure, flat, failures = _run_check_code(df, req.code)
        return VerifyCodeResponse(
            total=total,
            success=success,
            failure=failure,
            per_constraint=flat,
            failures=failures
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health():
    return {"status": "ok"}

# ===== å‰ç«¯ï¼ˆdist/ï¼‰é™æ€èµ„æº =====
# ç›®å½•ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ dist/
FRONTEND_DIR = (Path(__file__).resolve().parent.parent / "dist").resolve()

# æä¾› /assets ä¸‹çš„é™æ€èµ„æºï¼ˆJS/CSSï¼‰
assets_dir = FRONTEND_DIR / "assets"
if assets_dir.exists():
    app.mount("/assets", StaticFiles(directory=str(assets_dir)), name="assets")

# æ ¹è·¯å¾„è¿”å›ç¼–è¯‘åçš„ index.html
@app.get("/")
def serve_index():
    index_file = FRONTEND_DIR / "index.html"
    if not index_file.exists():
        # æ²¡æœ‰ dist æ—¶ç»™ä¸ªå‹å¥½æç¤º
        return {"message": "dist/ not found. Build your UI first."}
    return FileResponse(str(index_file))

# å¯é€‰ï¼šSPA fallbackï¼ˆæ”¯æŒå‰ç«¯è·¯ç”±ï¼Œé¿å…åˆ·æ–° 404ï¼‰
# æ³¨æ„ï¼š/suggestã€/generate ç­‰å·²ç²¾ç¡®åŒ¹é…ï¼Œä¸ä¼šè¢«æ­¤è·¯ç”±è¦†ç›–
@app.get("/{full_path:path}")
def spa_fallback(full_path: str):
    # å¦‚æœè¯·æ±‚çš„æ˜¯å·²æœ‰é™æ€æ–‡ä»¶ï¼Œå°±ç›´æ¥è¿”å›
    candidate = FRONTEND_DIR / full_path
    if candidate.exists() and candidate.is_file():
        return FileResponse(str(candidate))
    # å¦åˆ™è¿”å› index.htmlï¼Œç”±å‰ç«¯è·¯ç”±æ¥ç®¡
    index_file = FRONTEND_DIR / "index.html"
    if index_file.exists():
        return FileResponse(str(index_file))
    raise HTTPException(status_code=404, detail="Not Found")
