# api/main.py
import os
os.environ["JAVA_HOME"] = "/opt/homebrew/Cellar/openjdk@17/17.0.15/libexec/openjdk.jdk/Contents/Home"  # e.g. /usr/lib/jvm/java-11-openjdk-amd64 or $HOME/jdk/openjdk-11
os.environ["PATH"] = f"{os.environ['JAVA_HOME']}/bin:" + os.environ["PATH"]
os.environ["SPARK_VERSION"] = "3.5"

from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pyspark.sql import DataFrame
from pyspark.sql import SparkSession

from pyspark.sql import SparkSession, functions as F
from pydeequ.suggestions import ConstraintSuggestionRunner, DEFAULT
from pydeequ.checks import Check, CheckLevel
from pydeequ.verification import VerificationSuite, VerificationResult

app = FastAPI(title="Deequ Suggester API")
from starlette.middleware.base import BaseHTTPMiddleware

async def log_requests(request, call_next):
    print(f"🔵 {request.method} {request.url}")
    response = await call_next(request)
    print(f"🟢 {response.status_code}")
    return response

app.add_middleware(BaseHTTPMiddleware, dispatch=log_requests)

# CORS（开发阶段先放开；上线后改成你的前端域名）
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== Lazy Spark =====
_SPARK = None
def spark():
    global _SPARK
    if _SPARK is None:
        _SPARK = (
            SparkSession.builder
            .appName("PyDeequAPI")
            .config("spark.jars.packages", "com.amazon.deequ:deequ:2.0.9-spark-3.5")
            .getOrCreate()
        )
    return _SPARK

# ===== 请求/响应模型（精简版）=====
from pydantic import BaseModel

class SuggestRequest(BaseModel):
    path: str                          # 只要路径（按 Parquet 读取）
    key_cols: Optional[List[str]] = [] # 可选：拼个唯一键列

class CheckRow(BaseModel):
    id: str
    column: str
    description: Optional[str] = ""
    rule: Optional[str] = ""
    code: str
    include: bool = True
    current_value: Optional[str] = None

class GenerateRequest(BaseModel):
    rows: List[CheckRow]
    check_name: str = "AutoGenerated"
    level: str = "Error"  # or "Warning"

class VerifyCodeRequest(BaseModel):
    path: str          # 要验证的数据 Parquet 路径
    code: str          # /generate 返回的 final code 字符串

class VerifyCodeResponse(BaseModel):
    total: int
    success: int
    failure: int
    per_constraint: List[Dict[str, Any]]  # 按顺序列出每条约束的状态
    failures: List[Dict[str, Any]]

class GenerateAndVerifyOnceRequest(BaseModel):
    path: str
    key_cols: Optional[List[str]] = []
    rows: List[CheckRow]
    check_name: str = "AutoGenerated"
    level: str = "Error"

# ===== helpers =====
    
# ===== Simple DF cache (optional, but speeds things up) =====
DF_CACHE: Dict[str, DataFrame] = {}

def _load_df(req: SuggestRequest):
    sp = spark()
    df = sp.read.parquet(req.path)
    if req.key_cols:
        cols = [c for c in req.key_cols if c in df.columns]
        if cols:
            df = df.withColumn("Key_col", F.concat_ws("_", *[F.col(c) for c in cols]))
    # cache by path
    DF_CACHE[req.path] = df
    return df

def _group_suggestions(
    df,                      # pyspark.sql.DataFrame
    suggestionResult: Dict[str, Any],
    key_cols: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    
    rows: List[Dict[str, Any]] = []

    # Existing Deequ suggestions
    for s in suggestionResult.get("constraint_suggestions", []):
        col = s.get("column_name") or "<DATASET>"
        rows.append({
            "id": f"{col}:{s.get('constraint_name')}",
            "column": col,
            "description": s.get("description"),
            "rule": s.get("rule_description") or s.get("suggesting_rule"),
            "code": s.get("code_for_constraint"),
            "current_value": s.get("current_value"),
            "include": True,
        })

    # Extra checks for key columns
    if key_cols:
        # Uniqueness: count of distinct non-null / total count
        uniqueness = (
            df.agg((F.countDistinct(F.col("Key_col")) / F.count(F.lit(1))).alias("uniqueness"))
            .collect()[0]["uniqueness"]
        )
        rows.append({
            'id': 'key_cols:UniquenessConstraint(Uniqueness(List(key_cols),None,None))',
            'column': 'key_cols',
            'description': "'key_cols' is unique",
            'rule': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint',
            'code': '.isUnique("key_cols")',
            'current_value': f'ApproxDistinctness: {uniqueness}',
            'include': True
        })

        # Completeness: count of non-null / total count
        completeness = (
            df.filter(F.col(col).isNotNull())
            .count() / df.count()
        )
        rows.append({
            'id': 'key_cols:CompletenessConstraint(Completeness(key_cols,None,None))',
            'column': 'key_cols',
            'description': "'key_cols' is not null",
            'rule': 'If a column is complete in the sample, we suggest a NOT NULL constraint',
            'code': '.isComplete("key_cols")',
            'current_value': f'Completeness: {completeness}',
            'include': True
        })
    
    # Add row count (size) check
    rows.append({
        'id': '<DATASET>:SizeConstraint(Size(None,None))',
        'column': 'DATASET',
        'description': 'Dataset should contain at least one row',
        'rule': 'we suggest adding a Size > 0 constraint',
        'code': '.hasSize(lambda x: x > 0)',
        'current_value': f'Size: {df.count()}',
        'include': True
    })

    # Schema
    schema_list = df.dtypes
    all_cols = [c for c, _ in schema_list]
    dtype_map = dict(schema_list)
    suggested_cols = list(dict.fromkeys([r.get("column") for r in rows if r.get("column")]))

    # For columns not included in Deequ auto suggestions at all, add a placeholder rule
    for col in all_cols:
        if col not in suggested_cols:
            rows.append({
                'id': f'{col}',
                'column': f'{col}',
                'description': f"'{col}' no defined rules",
                'rule': ' ',
                'code': ' ',
                'current_value': ' ',
                'include': True
            })

    return rows

def _run_check_code(df, final_code: Any):
    """
    Execute Deequ checks against df.

    final_code can be:
      - a string like:
          "Check(spark, CheckLevel.Error, 'Name')\n    .isComplete('A')\n    .isNonNegative('AMOUNT')"
      - OR a pydeequ.checks.Check object (already constructed)
    Returns: total, success, failure, flat_results, failures_only
    """
    # If it's a string, eval it into a Check object (safe, restricted context)
    if isinstance(final_code, str):
        ctx = {
            "Check": Check,
            "CheckLevel": CheckLevel,
            "spark": spark,   # NOTE: in your generated code you call: Check(spark, ...)
        }
        try:
            check = eval(final_code, {}, ctx)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Invalid generated code: {e}")
    elif isinstance(final_code, Check):
        check = final_code
    else:
        raise HTTPException(status_code=400, detail="final_code must be a string or a pydeequ Check")

    # Run verification
    result = (
        VerificationSuite(spark())
        .onData(df)
        .addCheck(check)
        .run()
    )
    res_json = VerificationResult.checkResultsAsJson(spark(), result)

    flat: List[Dict[str, Any]] = []
    for chk in res_json.get("checkResults", []):
        flat.extend(chk.get("constraintResults", []))

    total = len(flat)
    success = sum(1 for c in flat if c.get("status") == "Success")
    failure = total - success

    failures: List[Dict[str, Any]] = []
    for c in flat:
        if c.get("status") == "Failure":
            failures.append({
                "constraint": c.get("constraint", ""),
                "column": c.get("column", ""),
                "message": c.get("message", ""),
                "metric": c.get("metric", ""),
                "actualValue": c.get("actualValue", ""),
            })

    return total, success, failure, flat, failures

# ===== API 路由 =====
@app.post("/suggest")
def suggest(req: SuggestRequest):
    try:
        df = _load_df(req)
        suggestionResult = (
            ConstraintSuggestionRunner(spark())
            .onData(df)
            .addConstraintRule(DEFAULT())
            .run()
        )
        rows = _group_suggestions(df, suggestionResult, key_cols = req.key_cols)
        schema = [{"name": f.name, "type": f.dataType.simpleString()} for f in df.schema.fields]
        return {"rows": rows, "row_count": len(rows), "schema": schema}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate")
def generate(req: GenerateRequest):
    selected = [r for r in req.rows if r.include and r.code]
    if not selected:
        return {"code": f"Check(spark, CheckLevel.{req.level}, {req.check_name!r})"}
    joined = "\n    ".join(r.code.strip() for r in selected)
    final_code = f"Check(spark, CheckLevel.{req.level}, {req.check_name!r})\n    {joined}"
    return {"code": final_code}

@app.post("/verify_code", response_model=VerifyCodeResponse)
def verify_code(req: VerifyCodeRequest):
    try:
        df = DF_CACHE.get(req.path)
        total, success, failure, flat, failures = _run_check_code(df, req.code)
        return VerifyCodeResponse(
            total=total,
            success=success,
            failure=failure,
            per_constraint=flat,
            failures=failures
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health():
    return {"status": "ok"}

# ===== 前端（dist/）静态资源 =====
# 目录：项目根目录下的 dist/
FRONTEND_DIR = (Path(__file__).resolve().parent.parent / "dist").resolve()

# 提供 /assets 下的静态资源（JS/CSS）
assets_dir = FRONTEND_DIR / "assets"
if assets_dir.exists():
    app.mount("/assets", StaticFiles(directory=str(assets_dir)), name="assets")

# 根路径返回编译后的 index.html
@app.get("/")
def serve_index():
    index_file = FRONTEND_DIR / "index.html"
    if not index_file.exists():
        # 没有 dist 时给个友好提示
        return {"message": "dist/ not found. Build your UI first."}
    return FileResponse(str(index_file))

# 可选：SPA fallback（支持前端路由，避免刷新 404）
# 注意：/suggest、/generate 等已精确匹配，不会被此路由覆盖
@app.get("/{full_path:path}")
def spa_fallback(full_path: str):
    # 如果请求的是已有静态文件，就直接返回
    candidate = FRONTEND_DIR / full_path
    if candidate.exists() and candidate.is_file():
        return FileResponse(str(candidate))
    # 否则返回 index.html，由前端路由接管
    index_file = FRONTEND_DIR / "index.html"
    if index_file.exists():
        return FileResponse(str(index_file))
    raise HTTPException(status_code=404, detail="Not Found")
