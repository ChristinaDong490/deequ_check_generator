# api/main.py
import os
os.environ["JAVA_HOME"] = "/opt/homebrew/Cellar/openjdk@17/17.0.15/libexec/openjdk.jdk/Contents/Home"  # e.g. /usr/lib/jvm/java-11-openjdk-amd64 or $HOME/jdk/openjdk-11
os.environ["PATH"] = f"{os.environ['JAVA_HOME']}/bin:" + os.environ["PATH"]
os.environ["SPARK_VERSION"] = "3.5"

from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pyspark.sql import DataFrame
from pyspark.sql import SparkSession

from pyspark.sql import SparkSession, functions as F
from pydeequ.suggestions import ConstraintSuggestionRunner, DEFAULT
from pydeequ.checks import Check, CheckLevel
from pydeequ.verification import VerificationSuite, VerificationResult

app = FastAPI(title="Deequ Suggester API")
from starlette.middleware.base import BaseHTTPMiddleware

async def log_requests(request, call_next):
    print(f"ğŸ”µ {request.method} {request.url}")
    response = await call_next(request)
    print(f"ğŸŸ¢ {response.status_code}")
    return response

app.add_middleware(BaseHTTPMiddleware, dispatch=log_requests)

# CORSï¼ˆå¼€å‘é˜¶æ®µå…ˆæ”¾å¼€ï¼›ä¸Šçº¿åæ”¹æˆä½ çš„å‰ç«¯åŸŸåï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== Lazy Spark =====
_SPARK = None
def spark():
    global _SPARK
    if _SPARK is None:
        _SPARK = (
            SparkSession.builder
            .appName("PyDeequAPI")
            .config("spark.jars.packages", "com.amazon.deequ:deequ:2.0.9-spark-3.5")
            .getOrCreate()
        )
    return _SPARK

# ===== è¯·æ±‚/å“åº”æ¨¡å‹ï¼ˆç²¾ç®€ç‰ˆï¼‰=====
from pydantic import BaseModel

class SuggestRequest(BaseModel):
    path: str                          # åªè¦è·¯å¾„ï¼ˆæŒ‰ Parquet è¯»å–ï¼‰
    key_cols: Optional[List[str]] = [] # å¯é€‰ï¼šæ‹¼ä¸ªå”¯ä¸€é”®åˆ—

class CheckRow(BaseModel):
    id: str
    column: str
    description: Optional[str] = ""
    rule: Optional[str] = ""
    code: str
    include: bool = True
    current_value: Optional[str] = None

class GenerateRequest(BaseModel):
    rows: List[CheckRow]
    check_name: str = "AutoGenerated"
    level: str = "Error"  # or "Warning"

class VerifyCodeRequest(BaseModel):
    path: str          # è¦éªŒè¯çš„æ•°æ® Parquet è·¯å¾„
    code: str          # /generate è¿”å›çš„ final code å­—ç¬¦ä¸²

class VerifyCodeResponse(BaseModel):
    total: int
    success: int
    failure: int
    per_constraint: List[Dict[str, Any]]  # æŒ‰é¡ºåºåˆ—å‡ºæ¯æ¡çº¦æŸçš„çŠ¶æ€
    failures: List[Dict[str, Any]]

class GenerateAndVerifyOnceRequest(BaseModel):
    path: str
    key_cols: Optional[List[str]] = []
    rows: List[CheckRow]
    check_name: str = "AutoGenerated"
    level: str = "Error"

# ===== helpers =====
    
# ===== Simple DF cache (optional, but speeds things up) =====
DF_CACHE: Dict[str, DataFrame] = {}

def _load_df(req: SuggestRequest):
    sp = spark()
    df = sp.read.parquet(req.path)
    if req.key_cols:
        cols = [c for c in req.key_cols if c in df.columns]
        if cols:
            df = df.withColumn("Key_col", F.concat_ws("_", *[F.col(c) for c in cols]))
    # cache by path
    DF_CACHE[req.path] = df
    return df

def _group_suggestions(
    df,                      # pyspark.sql.DataFrame
    suggestionResult: Dict[str, Any],
    key_cols: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    
    rows: List[Dict[str, Any]] = []

    # Existing Deequ suggestions
    for s in suggestionResult.get("constraint_suggestions", []):
        col = s.get("column_name") or "<DATASET>"
        rows.append({
            "id": f"{col}:{s.get('constraint_name')}",
            "column": col,
            "description": s.get("description"),
            "rule": s.get("rule_description") or s.get("suggesting_rule"),
            "code": s.get("code_for_constraint"),
            "current_value": s.get("current_value"),
            "include": True,
        })

    # Extra checks for key columns
    if key_cols:
        # Uniqueness: count of distinct non-null / total count
        uniqueness = (
            df.agg((F.countDistinct(F.col("Key_col")) / F.count(F.lit(1))).alias("uniqueness"))
            .collect()[0]["uniqueness"]
        )
        rows.append({
            'id': 'key_cols:UniquenessConstraint(Uniqueness(List(key_cols),None,None))',
            'column': 'key_cols',
            'description': "'key_cols' is unique",
            'rule': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint',
            'code': '.isUnique("key_cols")',
            'current_value': f'ApproxDistinctness: {uniqueness}',
            'include': True
        })

        # Completeness: count of non-null / total count
        completeness = (
            df.filter(F.col(col).isNotNull())
            .count() / df.count()
        )
        rows.append({
            'id': 'key_cols:CompletenessConstraint(Completeness(key_cols,None,None))',
            'column': 'key_cols',
            'description': "'key_cols' is not null",
            'rule': 'If a column is complete in the sample, we suggest a NOT NULL constraint',
            'code': '.isComplete("key_cols")',
            'current_value': f'Completeness: {completeness}',
            'include': True
        })
    
    # Add row count (size) check
    rows.append({
        'id': '<DATASET>:SizeConstraint(Size(None,None))',
        'column': 'DATASET',
        'description': 'Dataset should contain at least one row',
        'rule': 'we suggest adding a Size > 0 constraint',
        'code': '.hasSize(lambda x: x > 0)',
        'current_value': f'Size: {df.count()}',
        'include': True
    })

    # Schema
    schema_list = df.dtypes
    all_cols = [c for c, _ in schema_list]
    dtype_map = dict(schema_list)
    suggested_cols = list(dict.fromkeys([r.get("column") for r in rows if r.get("column")]))

    # For columns not included in Deequ auto suggestions at all, add a placeholder rule
    for col in all_cols:
        if col not in suggested_cols:
            rows.append({
                'id': f'{col}',
                'column': f'{col}',
                'description': f"'{col}' no defined rules",
                'rule': ' ',
                'code': ' ',
                'current_value': ' ',
                'include': True
            })

    return rows

def _run_check_code(df, final_code: Any):
    """
    Execute Deequ checks against df.

    final_code can be:
      - a string like:
          "Check(spark, CheckLevel.Error, 'Name')\n    .isComplete('A')\n    .isNonNegative('AMOUNT')"
      - OR a pydeequ.checks.Check object (already constructed)
    Returns: total, success, failure, flat_results, failures_only
    """
    # If it's a string, eval it into a Check object (safe, restricted context)
    if isinstance(final_code, str):
        ctx = {
            "Check": Check,
            "CheckLevel": CheckLevel,
            "spark": spark,   # NOTE: in your generated code you call: Check(spark, ...)
        }
        try:
            check = eval(final_code, {}, ctx)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Invalid generated code: {e}")
    elif isinstance(final_code, Check):
        check = final_code
    else:
        raise HTTPException(status_code=400, detail="final_code must be a string or a pydeequ Check")

    # Run verification
    result = (
        VerificationSuite(spark())
        .onData(df)
        .addCheck(check)
        .run()
    )
    res_json = VerificationResult.checkResultsAsJson(spark(), result)

    flat: List[Dict[str, Any]] = []
    for chk in res_json.get("checkResults", []):
        flat.extend(chk.get("constraintResults", []))

    total = len(flat)
    success = sum(1 for c in flat if c.get("status") == "Success")
    failure = total - success

    failures: List[Dict[str, Any]] = []
    for c in flat:
        if c.get("status") == "Failure":
            failures.append({
                "constraint": c.get("constraint", ""),
                "column": c.get("column", ""),
                "message": c.get("message", ""),
                "metric": c.get("metric", ""),
                "actualValue": c.get("actualValue", ""),
            })

    return total, success, failure, flat, failures

# ===== API è·¯ç”± =====
@app.post("/suggest")
def suggest(req: SuggestRequest):
    try:
        df = _load_df(req)
        suggestionResult = (
            ConstraintSuggestionRunner(spark())
            .onData(df)
            .addConstraintRule(DEFAULT())
            .run()
        )
        rows = _group_suggestions(df, suggestionResult, key_cols = req.key_cols)
        schema = [{"name": f.name, "type": f.dataType.simpleString()} for f in df.schema.fields]
        return {"rows": rows, "row_count": len(rows), "schema": schema}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate")
def generate(req: GenerateRequest):
    selected = [r for r in req.rows if r.include and r.code]
    if not selected:
        return {"code": f"Check(spark, CheckLevel.{req.level}, {req.check_name!r})"}
    joined = "\n    ".join(r.code.strip() for r in selected)
    final_code = f"Check(spark, CheckLevel.{req.level}, {req.check_name!r})\n    {joined}"
    return {"code": final_code}

@app.post("/verify_code", response_model=VerifyCodeResponse)
def verify_code(req: VerifyCodeRequest):
    try:
        df = DF_CACHE.get(req.path)
        total, success, failure, flat, failures = _run_check_code(df, req.code)
        return VerifyCodeResponse(
            total=total,
            success=success,
            failure=failure,
            per_constraint=flat,
            failures=failures
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health():
    return {"status": "ok"}

# ===== å‰ç«¯ï¼ˆdist/ï¼‰é™æ€èµ„æº =====
# ç›®å½•ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ dist/
FRONTEND_DIR = (Path(__file__).resolve().parent.parent / "dist").resolve()

# æä¾› /assets ä¸‹çš„é™æ€èµ„æºï¼ˆJS/CSSï¼‰
assets_dir = FRONTEND_DIR / "assets"
if assets_dir.exists():
    app.mount("/assets", StaticFiles(directory=str(assets_dir)), name="assets")

# æ ¹è·¯å¾„è¿”å›ç¼–è¯‘åçš„ index.html
@app.get("/")
def serve_index():
    index_file = FRONTEND_DIR / "index.html"
    if not index_file.exists():
        # æ²¡æœ‰ dist æ—¶ç»™ä¸ªå‹å¥½æç¤º
        return {"message": "dist/ not found. Build your UI first."}
    return FileResponse(str(index_file))

# å¯é€‰ï¼šSPA fallbackï¼ˆæ”¯æŒå‰ç«¯è·¯ç”±ï¼Œé¿å…åˆ·æ–° 404ï¼‰
# æ³¨æ„ï¼š/suggestã€/generate ç­‰å·²ç²¾ç¡®åŒ¹é…ï¼Œä¸ä¼šè¢«æ­¤è·¯ç”±è¦†ç›–
@app.get("/{full_path:path}")
def spa_fallback(full_path: str):
    # å¦‚æœè¯·æ±‚çš„æ˜¯å·²æœ‰é™æ€æ–‡ä»¶ï¼Œå°±ç›´æ¥è¿”å›
    candidate = FRONTEND_DIR / full_path
    if candidate.exists() and candidate.is_file():
        return FileResponse(str(candidate))
    # å¦åˆ™è¿”å› index.htmlï¼Œç”±å‰ç«¯è·¯ç”±æ¥ç®¡
    index_file = FRONTEND_DIR / "index.html"
    if index_file.exists():
        return FileResponse(str(index_file))
    raise HTTPException(status_code=404, detail="Not Found")
